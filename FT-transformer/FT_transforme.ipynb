{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af57fb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: d:\\DSS5104\\data\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # 忽略警告信息\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 导入 rtdl_revisiting_models 包中的模型类\n",
    "from rtdl_revisiting_models import FTTransformer\n",
    "\n",
    "warnings.resetwarnings()\n",
    "\n",
    "# 设置运行设备为GPU（如可用）或CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 设置随机种子确保可复现性\n",
    "# （delu.random.seed 会同时为 numpy、random、torch 等设置种子）\n",
    "import delu\n",
    "delu.random.seed(999)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from dataloader import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2745469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data(dataset: str):\n",
    "#     \"\"\"\n",
    "#     Load and preprocess dataset.\n",
    "#     Supports 'adult' (Adult Income dataset) and 'california' (California Housing dataset).\n",
    "#     Returns a tuple: (data_dict, n_cont_features, cat_cardinalities, d_out, task_type)\n",
    "#     \"\"\"\n",
    "#     dataset = dataset.lower()\n",
    "#     # Load dataset\n",
    "#     if dataset.startswith(\"adult\"):\n",
    "#         # Adult dataset (binary classification)\n",
    "#         data_train,data_test = load_data(\"adult\")\n",
    "        \n",
    "#         X_train = data_train.drop(columns=['income'])\n",
    "#         y_train = data_train['income']\n",
    "#         X_val = data_test.drop(columns=['income'])\n",
    "#         y_val = data_test['income']\n",
    "#         # Convert target to binary 0/1\n",
    "#         y_train = (y_train == '>50K').astype(int)  # 1 for >50K, 0 for <=50K\n",
    "#         y_val = (y_val == '>50K').astype(int)  # 1 for >50K, 0 for <=50K\n",
    "#         task_type = \"classification\"\n",
    "        \n",
    "#     elif dataset.startswith(\"california\"):\n",
    "#         X_train, X_val, y_train, y_val = load_data(\"california\")\n",
    "#         # California housing dataset (regression)\n",
    "#         task_type = \"regression\"\n",
    "        \n",
    "#     elif dataset.startswith(\"higgs\"):\n",
    "#         # Higgs dataset (binary classification)\n",
    "#         X_train, X_val, y_train, y_val = load_data(\"higgs\")\n",
    "#         # Convert target to binary 0/1\n",
    "#         y_train = (y_train == 1).astype(int)  # 1 for signal, 0 for background\n",
    "#         y_val = (y_val == 1).astype(int)  # 1 for signal, 0 for background\n",
    "#         task_type = \"classification\"\n",
    "        \n",
    "#     elif dataset.startswith(\"churn\"):\n",
    "#         # Churn dataset (binary classification)\n",
    "#         X_train, X_val, y_train, y_val = load_data(\"churn\")\n",
    "#         # Convert target to binary 0/1\n",
    "#         y_train = (y_train == 'Yes').astype(int)\n",
    "#         y_val = (y_val == 'Yes').astype(int)  # 1 for Yes, 0 for No\n",
    "#         task_type = \"classification\"\n",
    "        \n",
    "#     elif dataset.startswith(\"creditcard\"):\n",
    "#         # Credit Card Fraud Detection dataset (binary classification)\n",
    "#         X_train, X_val, y_train, y_val = load_data(\"credit\")\n",
    "#         # Convert target to binary 0/1\n",
    "#         y_train = (y_train == 1).astype(int)\n",
    "#         y_val = (y_val == 1).astype(int)  # 1 for fraud, 0 for non-fraud\n",
    "#         task_type = \"classification\"\n",
    "        \n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "    \n",
    "#     # Identify categorical and continuous columns\n",
    "#     cat_cols = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "#     cont_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "#     # Label-encode categorical columns\n",
    "#     cat_cardinalities = []\n",
    "#     if cat_cols:\n",
    "#         for col in cat_cols:\n",
    "#             # Convert to categorical dtype and then to codes\n",
    "#             X_train[col] = X_train[col].astype('category')\n",
    "#             X_val[col] = X_val[col].astype('category')\n",
    "#             # Save cardinality (number of unique categories)\n",
    "#             cat_cardinalities.append(X_train[col].nunique())\n",
    "#             # Replace column with codes (0 ... n-1)\n",
    "#             X_train[col] = X_train[col].cat.codes\n",
    "#             X_val[col] = X_val[col].cat.codes\n",
    "#             # # If any -1 codes (from NaN), replace with a new category code at end\n",
    "#             # if (X[col] < 0).any():\n",
    "#             #     X[col] = X[col].replace(-1, X[col].nunique())  # treat missing as new category\n",
    "#             #     cat_cardinalities[-1] += 1\n",
    "    \n",
    "#     # All continuous features to float32\n",
    "#     X_train[cont_cols] = X_train[cont_cols].astype('float32')\n",
    "#     X_val[cont_cols] = X_val[cont_cols].astype('float32')\n",
    "#     # Convert target to appropriate type\n",
    "#     if task_type == \"classification\":\n",
    "#         # Classification (target as int64 for PyTorch)\n",
    "#         y_train = y_train.astype('int64')\n",
    "#         y_val = y_val.astype('int64')\n",
    "#         # Determine number of classes for output\n",
    "#         n_classes = y_train.nunique()\n",
    "#         d_out = 1 if n_classes == 2 else n_classes\n",
    "#     else:\n",
    "#         # Regression (target as float32)\n",
    "#         y_train = y_train.astype('float32')\n",
    "#         y_val = y_val.astype('float32')\n",
    "#         d_out = 1  # one output for regression\n",
    "    \n",
    "    \n",
    "#     # Feature scaling (fit on train, apply to all splits)\n",
    "#     scaler = StandardScaler().fit(X_train[cont_cols])\n",
    "#     X_train[cont_cols] = scaler.transform(X_train[cont_cols])\n",
    "#     X_val[cont_cols]   = scaler.transform(X_val[cont_cols])\n",
    "\n",
    "    \n",
    "#     # Convert features and targets to tensors on the chosen device\n",
    "#     def to_tensor(dataframe):\n",
    "#         # Separate cont and cat features and convert to torch tensor\n",
    "#         x_cont_tensor = torch.tensor(dataframe[cont_cols].values, dtype=torch.float32, device=device)\n",
    "#         if cat_cols:\n",
    "#             x_cat_tensor = torch.tensor(dataframe[cat_cols].values, dtype=torch.int64, device=device)\n",
    "#         else:\n",
    "#             x_cat_tensor = None\n",
    "#         return x_cont_tensor, x_cat_tensor\n",
    "    \n",
    "#     x_cont_train, x_cat_train = to_tensor(X_train)\n",
    "#     x_cont_val,   x_cat_val   = to_tensor(X_val)\n",
    "\n",
    "#     y_train_tensor = torch.tensor(y_train.values, device=device)\n",
    "#     y_val_tensor   = torch.tensor(y_val.values, device=device)\n",
    "\n",
    "#     # For binary classification, use float targets for BCE loss\n",
    "#     if task_type == \"classification\" and d_out == 1:\n",
    "#         y_train_tensor = y_train_tensor.float()\n",
    "#         y_val_tensor   = y_val_tensor.float()\n",
    "\n",
    "    \n",
    "#     # Package data into dictionaries for convenience\n",
    "#     data = {\n",
    "#         \"train\": {\"x_cont\": x_cont_train, \"x_cat\": x_cat_train, \"y\": y_train_tensor},\n",
    "#         \"val\":   {\"x_cont\": x_cont_val,   \"x_cat\": x_cat_val,   \"y\": y_val_tensor},\n",
    "#     }\n",
    "#     n_cont_features = len(cont_cols)\n",
    "#     return data, n_cont_features, cat_cardinalities, d_out, task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8477eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_cont_features: int, cat_cardinalities: list, d_out: int):\n",
    "    \"\"\"\n",
    "    Build an FT-Transformer model.\n",
    "    n_cont_features: number of continuous (numeric) features\n",
    "    cat_cardinalities: list of cardinalities for each categorical feature (empty if none)\n",
    "    d_out: dimension of model output (e.g. number of classes or 1)\n",
    "    \"\"\"\n",
    "    model = FTTransformer(\n",
    "        n_cont_features=n_cont_features,\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        d_out=d_out,\n",
    "        **FTTransformer.get_default_kwargs()  # use default recommended hyperparameters\n",
    "    ).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76f6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, data: dict, task_type: str, d_out: int,\n",
    "#                 n_epochs: int = 100, batch_size: int = 256, patience: int = 10, \n",
    "#                 lr: float = 3e-4, weight_decay: float = 1e-5):\n",
    "#     \"\"\"\n",
    "#     Train the model using the provided data.\n",
    "#     Returns a dict with best validation (and corresponding test) metrics and epoch.\n",
    "#     \"\"\"\n",
    "#     # Select appropriate loss function\n",
    "#     if task_type == \"classification\":\n",
    "#         # Binary classification vs multiclass\n",
    "#         if d_out == 1:\n",
    "#             loss_fn = F.binary_cross_entropy_with_logits  # expects logits and float targets\n",
    "#         else:\n",
    "#             loss_fn = F.cross_entropy                    # expects logits and class indices\n",
    "#     else:\n",
    "#         loss_fn = F.mse_loss  # regression\n",
    "    \n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     best_val_score = None  # best validation metric (accuracy or negative loss)\n",
    "#     best_epoch = -1\n",
    "#     best_state = None  # store best model parameters\n",
    "    \n",
    "#     # Determine if we maximize or minimize the val metric\n",
    "#     maximize_metric = True if task_type == \"classification\" else False\n",
    "    \n",
    "#     patience_counter = 0\n",
    "#     n_train = data[\"train\"][\"y\"].shape[0]\n",
    "    \n",
    "#     for epoch in range(1, n_epochs+1):\n",
    "#         model.train()\n",
    "#         # Shuffle training indices for each epoch\n",
    "#         indices = torch.randperm(n_train, device=device)\n",
    "#         total_loss = 0.0\n",
    "#         total_correct = 0\n",
    "#         total_samples = 0\n",
    "        \n",
    "#         # Mini-batch training\n",
    "#         for start in range(0, n_train, batch_size):\n",
    "#             end = start + batch_size\n",
    "#             batch_idx = indices[start:end]\n",
    "#             x_cont_batch = data[\"train\"][\"x_cont\"][batch_idx]\n",
    "#             y_batch = data[\"train\"][\"y\"][batch_idx]\n",
    "#             if data[\"train\"][\"x_cat\"] is not None:\n",
    "#                 x_cat_batch = data[\"train\"][\"x_cat\"][batch_idx]\n",
    "#                 logits = model(x_cont_batch, x_cat_batch)\n",
    "#             else:\n",
    "#                 logits = model(x_cont_batch, None)\n",
    "#             # For binary/regression, squeeze output to 1D tensor\n",
    "#             if task_type == \"classification\":\n",
    "#                 if d_out == 1:\n",
    "#                     logits = logits.squeeze(-1)  # shape (batch,)\n",
    "#             else:\n",
    "#                 logits = logits.squeeze(-1)      # regression output shape (batch,)\n",
    "#             loss = loss_fn(logits, y_batch)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Accumulate training loss\n",
    "#             # Multiply by batch size to aggregate (loss is mean per batch by default for BCE/MSE/XEnt)\n",
    "#             total_loss += loss.item() * len(batch_idx)\n",
    "#             total_samples += len(batch_idx)\n",
    "#             # Accumulate correct predictions for classification\n",
    "#             if task_type == \"classification\":\n",
    "#                 if d_out == 1:\n",
    "#                     # Binary classification: predict label 0/1 by thresholding logits\n",
    "#                     preds = (logits > 0).long()\n",
    "#                     targets = y_batch.long()  # y_batch is float for BCE, convert to long for comparison\n",
    "#                 else:\n",
    "#                     # Multiclass: pick class with highest logit\n",
    "#                     preds = torch.argmax(logits, dim=1)\n",
    "#                     targets = y_batch\n",
    "#                 total_correct += (preds == targets).sum().item()\n",
    "        \n",
    "#         # Compute average training loss and accuracy\n",
    "#         avg_train_loss = total_loss / total_samples\n",
    "#         if task_type == \"classification\":\n",
    "#             train_acc = total_correct / total_samples\n",
    "#         else:\n",
    "#             train_acc = None  # not applicable for regression\n",
    "        \n",
    "#         # Evaluate on validation and test sets\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             # Validation set\n",
    "#             x_cont_val = data[\"val\"][\"x_cont\"]\n",
    "#             y_val = data[\"val\"][\"y\"]\n",
    "#             if data[\"val\"][\"x_cat\"] is not None:\n",
    "#                 x_cat_val = data[\"val\"][\"x_cat\"]\n",
    "#                 val_logits = model(x_cont_val, x_cat_val)\n",
    "#             else:\n",
    "#                 val_logits = model(x_cont_val, None)\n",
    "#             if task_type == \"classification\":\n",
    "#                 if d_out == 1:\n",
    "#                     val_logits = val_logits.squeeze(-1)\n",
    "#                     val_loss = F.binary_cross_entropy_with_logits(val_logits, y_val)\n",
    "#                     # Convert logits to predicted labels for accuracy\n",
    "#                     val_preds = (val_logits > 0).long()\n",
    "#                     val_targets = y_val.long()\n",
    "#                     val_correct = (val_preds == val_targets).sum().item()\n",
    "#                     val_acc = val_correct / len(y_val)\n",
    "#                 else:\n",
    "#                     # Multiclass\n",
    "#                     val_loss = F.cross_entropy(val_logits, y_val)\n",
    "#                     val_preds = torch.argmax(val_logits, dim=1)\n",
    "#                     val_correct = (val_preds == y_val).sum().item()\n",
    "#                     val_acc = val_correct / len(y_val)\n",
    "#             else:\n",
    "#                 # Regression\n",
    "#                 val_logits = val_logits.squeeze(-1)\n",
    "#                 val_loss = F.mse_loss(val_logits, y_val)\n",
    "#                 val_acc = None  # no accuracy for regression (we will use loss for early stopping)\n",
    "        \n",
    "#         # Determine the metric to use for early stopping and best model tracking\n",
    "#         if task_type == \"classification\":\n",
    "#             current_val_score = val_acc\n",
    "#         else:\n",
    "#             current_val_score = -val_loss.item()  # use negative loss so that \"higher is better\" (to simplify logic)\n",
    "        \n",
    "#         # Check improvement for early stopping\n",
    "#         if best_val_score is None or current_val_score > best_val_score:\n",
    "#             best_val_score = current_val_score\n",
    "#             best_epoch = epoch\n",
    "#             # Save model state at best epoch\n",
    "#             best_state = copy.deepcopy(model.state_dict())\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "        \n",
    "#         # Logging epoch results\n",
    "#         if task_type == \"classification\":\n",
    "#             print(f\"Epoch {epoch:03d}: \"\n",
    "#                   f\"Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.4f}, \"\n",
    "#                   f\"Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc:.4f}, \")\n",
    "#         else:\n",
    "#             # For regression, report RMSE for interpretability\n",
    "#             train_rmse = np.sqrt(avg_train_loss)\n",
    "#             val_rmse = np.sqrt(val_loss.item())\n",
    "#             print(f\"Epoch {epoch:03d}: \"\n",
    "#                   f\"Train MSE = {avg_train_loss:.4f} (RMSE={train_rmse:.4f}), \"\n",
    "#                   f\"Val MSE = {val_loss.item():.4f} (RMSE={val_rmse:.4f})\")\n",
    "        \n",
    "#         # Early stopping check\n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping triggered after epoch {epoch}.\")\n",
    "#             break\n",
    "        \n",
    "#         torch.cuda.empty_cache()  # 在每个Epoch结束后调用，释放未使用的显存\n",
    "    \n",
    "#     # Restore best model weights (early stopping uses the model at best epoch)\n",
    "#     if best_state is not None:\n",
    "#         model.load_state_dict(best_state)\n",
    "#         torch.save(model.state_dict(), \"FT_best_model.pth\")\n",
    "    \n",
    "#     # Prepare results\n",
    "#     if task_type == \"classification\":\n",
    "#         best_val_acc = best_val_score  # since best_val_score holds actual accuracy\n",
    "#         result = {\n",
    "#             \"best_epoch\": best_epoch,\n",
    "#             \"best_val_acc\": float(best_val_acc),\n",
    "#         }\n",
    "#         print(f\"Training completed. Best epoch = {best_epoch}, Best Val Accuracy = {best_val_acc:.4f}\")\n",
    "#     else:\n",
    "#         # Convert stored negative loss back to positive MSE\n",
    "#         best_val_mse = -best_val_score if best_val_score is not None else None\n",
    "#         result = {\n",
    "#             \"best_epoch\": best_epoch,\n",
    "#             \"best_val_mse\": float(best_val_mse) if best_val_mse is not None else None,\n",
    "#         }\n",
    "#         if best_val_mse is not None:\n",
    "#             print(f\"Training completed. Best epoch = {best_epoch}, Best Val RMSE = {np.sqrt(best_val_mse):.4f}\")\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fe654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage for Adult dataset (binary classification)\n",
    "# data, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"adult\")\n",
    "# model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "# results = train_model(model, data, task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "# print(\"Adult Dataset Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b613a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage for California Housing dataset (regression)\n",
    "# data, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"california\")\n",
    "# model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "# results = train_model(model, data, task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "# print(\"California Housing Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933d2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage for Higgs dataset (binary classification)\n",
    "# data, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"higgs\")\n",
    "# model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "# results = train_model(model, data, task_type, d_out, n_epochs=100, batch_size=32, patience=10, lr=3e-4)\n",
    "# print(\"Higgs Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e74fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage for Credit Card Fraud Detection dataset (binary classification)\n",
    "# data, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"creditcard\")\n",
    "# model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "# results = train_model(model, data, task_type, d_out, n_epochs=100, batch_size=64, patience=10, lr=3e-4)\n",
    "# print(\"Credit Card Fraud Detection Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0b5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X_df, y, cat_cols, cont_cols, task_type, is_train=False, scaler=None, cat_categories=None):\n",
    "        \"\"\"\n",
    "        初始化Dataset，执行数据预处理（类别编码、数值缩放等）。\n",
    "        X_df: pandas DataFrame，特征数据\n",
    "        y: pandas Series或numpy数组，目标数据\n",
    "        cat_cols: 类别特征列名列表\n",
    "        cont_cols: 连续特征列名列表\n",
    "        task_type: \"classification\" 或 \"regression\"，任务类型\n",
    "        is_train: 是否为训练集（训练集会拟合Scaler等）\n",
    "        scaler: 训练集拟合的StandardScaler（验证集/测试集传入同一个Scaler以保证一致性）\n",
    "        cat_categories: 可选，训练集每个类别特征的类别列表（用于在验证集上保持类别编码一致）\n",
    "        \"\"\"\n",
    "        self.cat_cols = cat_cols\n",
    "        self.cont_cols = cont_cols\n",
    "        self.task_type = task_type\n",
    "        # 复制一份数据，避免修改原始DataFrame\n",
    "        X = X_df.copy()\n",
    "        \n",
    "        # 类别型特征处理：转换为categorical类型并编码\n",
    "        self.cat_cardinalities = []      # 保存每个类别特征的基数（unique个数）\n",
    "        self.cat_categories = {}         # 保存训练集中每个类别特征的类别值列表\n",
    "        if self.cat_cols:\n",
    "            for col in self.cat_cols:\n",
    "                if is_train:\n",
    "                    # 训练集：将特征转换为categorical并获取类别列表\n",
    "                    X[col] = X[col].astype('category')\n",
    "                    self.cat_categories[col] = X[col].cat.categories  # 保存类别值\n",
    "                    self.cat_cardinalities.append(X[col].nunique())   # 唯一值数量作为类别基数\n",
    "                else:\n",
    "                    # 验证/测试集：若提供了训练集的类别列表，则使用它保证编码一致\n",
    "                    if cat_categories is not None and col in cat_categories:\n",
    "                        X[col] = pd.Categorical(X[col], categories=cat_categories[col])\n",
    "                    else:\n",
    "                        X[col] = X[col].astype('category')\n",
    "                # 将类别值映射为编码 (0,...,n-1)，缺失或未知类别将被编码为 -1\n",
    "                X[col] = X[col].cat.codes\n",
    "\n",
    "        \n",
    "        # 连续型特征处理：转换类型并标准化\n",
    "        self.scaler = None\n",
    "        if self.cont_cols:\n",
    "            # 确保连续特征为float32类型\n",
    "            X[self.cont_cols] = X[self.cont_cols].astype('float32')\n",
    "            if is_train:\n",
    "                # 拟合StandardScaler并应用于训练数据\n",
    "                self.scaler = StandardScaler()\n",
    "                X[self.cont_cols] = self.scaler.fit_transform(X[self.cont_cols])\n",
    "            else:\n",
    "                # 使用训练集的Scaler对验证/测试集进行变换\n",
    "                X[self.cont_cols] = scaler.transform(X[self.cont_cols])\n",
    "        \n",
    "        # 保存处理后的特征为Tensor\n",
    "        if self.cont_cols:\n",
    "            # 连续特征转换为浮点Tensor\n",
    "            self.X_cont = torch.tensor(X[self.cont_cols].values, dtype=torch.float32)\n",
    "        else:\n",
    "            # 若没有连续特征，则用None占位\n",
    "            self.X_cont = None\n",
    "        if self.cat_cols:\n",
    "            # 类别特征转换为长整型Tensor\n",
    "            self.X_cat = torch.tensor(X[self.cat_cols].values, dtype=torch.long)\n",
    "        else:\n",
    "            self.X_cat = None\n",
    "        \n",
    "        # 目标变量处理：根据任务类型选择dtype\n",
    "        # 对于分类任务，默认使用long张量存储类别（若二分类且使用BCELoss，后续会转换为float）\n",
    "        # 对于回归任务，使用float张量\n",
    "        y_array = np.array(y)  # 将Series转换为numpy数组\n",
    "        if task_type == \"classification\":\n",
    "            # 检查y的数据类型，若已经是浮点（表示二分类），则用float32，否则用long\n",
    "            target_dtype = torch.float32 if str(y_array.dtype).startswith('float') else torch.long\n",
    "            self.y = torch.tensor(y_array, dtype=target_dtype)\n",
    "        else:\n",
    "            self.y = torch.tensor(y_array, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 返回数据集样本数量\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引idx返回一个样本的特征和标签\n",
    "        # 提取连续特征，如果没有连续特征则返回空Tensor\n",
    "        x_cont = self.X_cont[idx] if self.X_cont is not None else torch.tensor([], dtype=torch.float32)\n",
    "        # 提取类别特征，如果没有类别特征则返回None\n",
    "        x_cat = self.X_cat[idx] if self.X_cat is not None else None\n",
    "        y = self.y[idx]\n",
    "        return x_cont, x_cat, y\n",
    "\n",
    "# 定义自定义的collate_fn函数，处理批次数据的合并（特别是处理x_cat可能为None的情况）\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一批(batch)的列表样本合并成一个批次输出。\n",
    "    batch参数是一个列表，内含若干来自Dataset的__getitem__返回的元组 (x_cont, x_cat, y)。\n",
    "    该函数将这些样本堆叠成批次张量，并处理None的情况。\n",
    "    \"\"\"\n",
    "    x_cont_list, x_cat_list, y_list = [], [], []\n",
    "    for (x_cont, x_cat, y) in batch:\n",
    "        # 将连续特征和标签加入列表\n",
    "        x_cont_list.append(x_cont)\n",
    "        y_list.append(y)\n",
    "        # 将类别特征加入列表（如果存在）\n",
    "        if x_cat is not None:\n",
    "            x_cat_list.append(x_cat)\n",
    "    # 将列表堆叠为tensor\n",
    "    x_cont_batch = torch.stack(x_cont_list) if len(x_cont_list) > 0 else None\n",
    "    y_batch = torch.stack(y_list)\n",
    "    # 类别特征列表可能为空（表示没有类别特征），注意区分处理\n",
    "    if len(x_cat_list) > 0:\n",
    "        x_cat_batch = torch.stack(x_cat_list)\n",
    "    else:\n",
    "        x_cat_batch = None\n",
    "    return x_cont_batch, x_cat_batch, y_batch\n",
    "\n",
    "# 使用Dataset和DataLoader改写数据加载流程\n",
    "def prepare_data(dataset: str,batch_size: int = 256):   \n",
    "    dataset = dataset.lower()\n",
    "    # 根据数据集名称加载数据并进行初步处理（划分训练/验证集，编码标签等）\n",
    "    if dataset.startswith(\"adult\"):\n",
    "        # Adult数据集（二分类）\n",
    "        data_train, data_test = load_data(\"adult\")\n",
    "        X_train = data_train.drop(columns=['income'])\n",
    "        y_train = data_train['income']\n",
    "        X_val = data_test.drop(columns=['income'])\n",
    "        y_val = data_test['income']\n",
    "        # 将收入标签转换为0/1（二分类）\n",
    "        y_train = (y_train == '>50K').astype(int)\n",
    "        y_val = (y_val == '>50K').astype(int)\n",
    "        task_type = \"classification\"\n",
    "    elif dataset.startswith(\"california\"):\n",
    "        # 加州房价数据集（回归）\n",
    "        X_train, X_val, y_train, y_val = load_data(\"california\")\n",
    "        task_type = \"regression\"\n",
    "    elif dataset.startswith(\"higgs\"):\n",
    "        # Higgs数据集（二分类）\n",
    "        X_train, X_val, y_train, y_val = load_data(\"higgs\")\n",
    "        # 将标签转换为0/1（二分类：1表示signal，0表示background）\n",
    "        y_train = (y_train == 1).astype(int)\n",
    "        y_val = (y_val == 1).astype(int)\n",
    "        task_type = \"classification\"\n",
    "    elif dataset.startswith(\"churn\"):\n",
    "        # 用户流失(Churn)数据集（二分类）\n",
    "        X_train, X_val, y_train, y_val = load_data(\"churn\")\n",
    "        # 将标签转换为0/1（'Yes'->1表示流失，'No'->0表示未流失）\n",
    "        y_train = (y_train == 'Yes').astype(int)\n",
    "        y_val = (y_val == 'Yes').astype(int)\n",
    "        task_type = \"classification\"\n",
    "    elif dataset.startswith(\"creditcard\"):\n",
    "        # 信用卡欺诈检测数据集（二分类）\n",
    "        X_train, X_val, y_train, y_val = load_data(\"credit\")\n",
    "        # 将标签转换为0/1（1表示欺诈，0表示正常）\n",
    "        y_train = (y_train == 1).astype(int)\n",
    "        y_val = (y_val == 1).astype(int)\n",
    "        task_type = \"classification\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "    \n",
    "    # 确定类别和连续特征列\n",
    "    cat_cols = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "    cont_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # 根据任务类型转换目标数据类型（classification用int64，regression用float32）\n",
    "    if task_type == \"classification\":\n",
    "        y_train = y_train.astype('int64')\n",
    "        y_val = y_val.astype('int64')\n",
    "        # 判断是否为二分类任务，以决定输出维度和目标张量类型\n",
    "        n_classes = pd.Series(y_train).nunique()  # 类别数量\n",
    "        d_out = 1 if n_classes == 2 else n_classes\n",
    "        if d_out == 1:\n",
    "            # 二分类任务将标签转为float32，以便使用BCE损失\n",
    "            y_train = y_train.astype('float32')\n",
    "            y_val = y_val.astype('float32')\n",
    "    else:\n",
    "        # 回归任务，将标签转为float32\n",
    "        y_train = y_train.astype('float32')\n",
    "        y_val = y_val.astype('float32')\n",
    "        d_out = 1\n",
    "    \n",
    "    # 创建训练Dataset（在初始化时会执行特征编码和标准化）\n",
    "    train_dataset = TabularDataset(X_train, y_train, cat_cols, cont_cols, task_type, is_train=True)\n",
    "    # 创建验证Dataset，使用训练集的Scaler以保持相同的特征缩放\n",
    "    val_dataset = TabularDataset(X_val, y_val, cat_cols, cont_cols, task_type, is_train=False,\n",
    "                                 scaler=train_dataset.scaler, cat_categories=train_dataset.cat_categories)\n",
    "    \n",
    "    # 使用DataLoader创建训练和验证集的迭代器\n",
    "    # 可以根据需要调整batch_size，下面设置一个默认值\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # 提取特征维度和类别基数等信息供模型构建使用\n",
    "    n_cont_features = len(cont_cols)                          # 连续特征数量\n",
    "    cat_cardinalities = train_dataset.cat_cardinalities       # 每个类别特征的基数列表\n",
    "    task_type = task_type\n",
    "    # d_out 已根据任务类型计算\n",
    "    \n",
    "    # 返回包含DataLoader的字典，以及特征和任务信息\n",
    "    data_loaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "    return data_loaders, n_cont_features, cat_cardinalities, d_out, task_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc505c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, task_type, d_out, \n",
    "                n_epochs=100, batch_size=256, patience=10, lr=3e-4, weight_decay=1e-5):\n",
    "    \"\"\"\n",
    "    训练模型，使用提供的训练和验证DataLoader按批次加载数据。\n",
    "    返回包含最佳验证结果的字典和对应的epoch。\n",
    "    \"\"\"\n",
    "    # 根据任务类型选择合适的损失函数\n",
    "    if task_type == \"classification\":\n",
    "        loss_fn = F.binary_cross_entropy_with_logits if d_out == 1 else F.cross_entropy\n",
    "    else:\n",
    "        loss_fn = F.mse_loss  # 回归任务使用均方误差损失\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_score = None\n",
    "    best_epoch = -1\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    # 判断验证指标是最大化（分类准确率）还是最小化（回归损失）\n",
    "    maximize_metric = True if task_type == \"classification\" else False\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        # 遍历训练集DataLoader，每次获取一个批次的数据\n",
    "        for x_cont_batch, x_cat_batch, y_batch in train_loader:\n",
    "            # 将批次数据移动到计算设备(device)上\n",
    "            x_cont_batch = x_cont_batch.to(device) if x_cont_batch is not None else None\n",
    "            x_cat_batch = x_cat_batch.to(device) if x_cat_batch is not None else None\n",
    "            y_batch = y_batch.to(device)\n",
    "            # 前向传播：根据是否存在类别特征选择模型输入\n",
    "            logits = model(x_cont_batch, x_cat_batch) if x_cat_batch is not None else model(x_cont_batch, None)\n",
    "            # 对于二分类或回归，需要将输出logits变形为一维\n",
    "            if task_type == \"classification\" and d_out == 1:\n",
    "                logits = logits.squeeze(-1)  # 二分类，形状(batch,)\n",
    "            elif task_type == \"regression\":\n",
    "                logits = logits.squeeze(-1)  # 回归，形状(batch,)\n",
    "            # 计算损失\n",
    "            # 注：若为二分类，loss_fn=BCELoss，此时y_batch应为float类型\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 累积训练损失和样本数（loss默认是batch均值，因此乘以批大小）\n",
    "            batch_size_actual = y_batch.size(0)\n",
    "            total_loss += loss.item() * batch_size_actual\n",
    "            total_samples += batch_size_actual\n",
    "            # 累积正确预测数量（仅分类任务需要）\n",
    "            if task_type == \"classification\":\n",
    "                if d_out == 1:\n",
    "                    # 二分类：logits大于0视为预测正类(1)\n",
    "                    preds = (logits > 0).long()\n",
    "                    targets = y_batch.long()  # 将目标转换为long以便比较\n",
    "                else:\n",
    "                    # 多分类：取最大logit对应的类别\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    targets = y_batch\n",
    "                total_correct += (preds == targets).sum().item()\n",
    "        # 计算平均训练损失\n",
    "        avg_train_loss = total_loss / total_samples\n",
    "        # 计算训练准确率（分类任务）\n",
    "        train_acc = total_correct / total_samples if task_type == \"classification\" else None\n",
    "        \n",
    "        # 在验证集上评估\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x_cont_val, x_cat_val, y_val in val_loader:\n",
    "                x_cont_val = x_cont_val.to(device) if x_cont_val is not None else None\n",
    "                x_cat_val = x_cat_val.to(device) if x_cat_val is not None else None\n",
    "                y_val = y_val.to(device)\n",
    "                # 前向传播得到验证集预测\n",
    "                val_logits = model(x_cont_val, x_cat_val) if x_cat_val is not None else model(x_cont_val, None)\n",
    "                if task_type == \"classification\" and d_out == 1:\n",
    "                    val_logits = val_logits.squeeze(-1)\n",
    "                elif task_type == \"regression\":\n",
    "                    val_logits = val_logits.squeeze(-1)\n",
    "                # 计算验证损失\n",
    "                val_loss = loss_fn(val_logits, y_val)\n",
    "                # 累积验证损失和样本数\n",
    "                batch_val_size = y_val.size(0)\n",
    "                val_loss_total += val_loss.item() * batch_val_size\n",
    "                total_val_samples += batch_val_size\n",
    "                # 累积验证集上的正确预测数（分类任务）\n",
    "                if task_type == \"classification\":\n",
    "                    if d_out == 1:\n",
    "                        # 二分类准确率计算\n",
    "                        val_preds = (val_logits > 0).long()\n",
    "                        val_targets = y_val.long()\n",
    "                    else:\n",
    "                        # 多分类准确率计算\n",
    "                        val_preds = torch.argmax(val_logits, dim=1)\n",
    "                        val_targets = y_val\n",
    "                    val_correct += (val_preds == val_targets).sum().item()\n",
    "        # 计算平均验证损失和准确率\n",
    "        avg_val_loss = val_loss_total / total_val_samples\n",
    "        if task_type == \"classification\":\n",
    "            val_acc = val_correct / total_val_samples\n",
    "        else:\n",
    "            val_acc = None  # 回归任务无需准确率\n",
    "        \n",
    "        # 根据任务类型确定当前验证指标（分类使用准确率，回归使用损失的负值）\n",
    "        current_val_score = val_acc if task_type == \"classification\" else -avg_val_loss\n",
    "        # Early Stopping: 检查验证指标是否改进\n",
    "        if best_val_score is None or (maximize_metric and current_val_score > best_val_score) or (not maximize_metric and current_val_score < best_val_score):\n",
    "            best_val_score = current_val_score\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # 打印当前轮次的训练和验证结果\n",
    "        if task_type == \"classification\":\n",
    "            print(f\"Epoch {epoch:03d}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.4f}, \"\n",
    "                  f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "        else:\n",
    "            # 对于回归问题，计算RMSE便于解释\n",
    "            train_rmse = np.sqrt(avg_train_loss)\n",
    "            val_rmse = np.sqrt(avg_val_loss)\n",
    "            print(f\"Epoch {epoch:03d}: Train MSE = {avg_train_loss:.4f} (RMSE={train_rmse:.4f}), \"\n",
    "                  f\"Val MSE = {avg_val_loss:.4f} (RMSE={val_rmse:.4f})\")\n",
    "        \n",
    "        # 若超过若干轮次未提升，则提前停止训练\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    # 恢复模型在验证集上最佳的状态\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    # 返回最佳验证指标（以及对应测试集指标）的结果和最佳轮次\n",
    "    results = {\"best_epoch\": best_epoch}\n",
    "    if task_type == \"classification\":\n",
    "        results[\"best_val_acc\"] = val_acc if best_val_score == val_acc else (best_val_score if maximize_metric else None)\n",
    "    else:\n",
    "        results[\"best_val_loss\"] = avg_val_loss if best_val_score == -avg_val_loss else (-best_val_score if not maximize_metric else None)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5372f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # churn dataset\n",
    "# data_loaders, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"churn\", batch_size=256)\n",
    "# model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "# results = train_model(model, data_loaders[\"train\"], data_loaders[\"val\"], task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "# print(\"Churn Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f6f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classification\n",
      "(30162, 15) (30162,)\n",
      "(15060, 15) (15060,)\n",
      "Epoch 001: Train Loss = 0.3453, Train Acc = 0.8414, Val Loss = 0.3176, Val Acc = 0.8509\n",
      "Epoch 002: Train Loss = 0.3178, Train Acc = 0.8525, Val Loss = 0.3184, Val Acc = 0.8563\n",
      "Epoch 003: Train Loss = 0.3131, Train Acc = 0.8549, Val Loss = 0.3157, Val Acc = 0.8520\n",
      "Epoch 004: Train Loss = 0.3112, Train Acc = 0.8558, Val Loss = 0.3108, Val Acc = 0.8550\n",
      "Epoch 005: Train Loss = 0.3079, Train Acc = 0.8567, Val Loss = 0.3131, Val Acc = 0.8539\n",
      "Epoch 006: Train Loss = 0.3091, Train Acc = 0.8560, Val Loss = 0.3118, Val Acc = 0.8534\n",
      "Epoch 007: Train Loss = 0.3074, Train Acc = 0.8555, Val Loss = 0.3145, Val Acc = 0.8562\n",
      "Epoch 008: Train Loss = 0.3040, Train Acc = 0.8583, Val Loss = 0.3140, Val Acc = 0.8552\n",
      "Epoch 009: Train Loss = 0.3029, Train Acc = 0.8602, Val Loss = 0.3213, Val Acc = 0.8492\n",
      "Epoch 010: Train Loss = 0.3024, Train Acc = 0.8594, Val Loss = 0.3092, Val Acc = 0.8560\n",
      "Epoch 011: Train Loss = 0.3006, Train Acc = 0.8613, Val Loss = 0.3132, Val Acc = 0.8533\n",
      "Epoch 012: Train Loss = 0.3003, Train Acc = 0.8614, Val Loss = 0.3107, Val Acc = 0.8529\n",
      "Early stopping triggered.\n",
      "Churn Results: {'best_epoch': 2, 'best_val_acc': 0.8563081009296148}\n"
     ]
    }
   ],
   "source": [
    "# adult dataset\n",
    "data_loaders, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"adult\", batch_size=256)\n",
    "model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "results = train_model(model, data_loaders[\"train\"], data_loaders[\"val\"], task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "print(\"Churn Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd1bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression\n",
      "(16512, 8) (16512,)\n",
      "(4128, 8) (4128,)\n",
      "Epoch 001: Train MSE = 0.7261 (RMSE=0.8521), Val MSE = 0.5238 (RMSE=0.7238)\n",
      "Epoch 002: Train MSE = 0.3882 (RMSE=0.6231), Val MSE = 0.3421 (RMSE=0.5849)\n",
      "Epoch 003: Train MSE = 0.3466 (RMSE=0.5888), Val MSE = 0.3519 (RMSE=0.5932)\n",
      "Epoch 004: Train MSE = 0.3350 (RMSE=0.5788), Val MSE = 0.3578 (RMSE=0.5982)\n",
      "Epoch 005: Train MSE = 0.3242 (RMSE=0.5694), Val MSE = 0.3163 (RMSE=0.5624)\n",
      "Epoch 006: Train MSE = 0.3130 (RMSE=0.5595), Val MSE = 0.3107 (RMSE=0.5574)\n",
      "Epoch 007: Train MSE = 0.3126 (RMSE=0.5591), Val MSE = 0.3279 (RMSE=0.5726)\n",
      "Epoch 008: Train MSE = 0.3049 (RMSE=0.5522), Val MSE = 0.2981 (RMSE=0.5460)\n",
      "Epoch 009: Train MSE = 0.2929 (RMSE=0.5412), Val MSE = 0.3057 (RMSE=0.5529)\n",
      "Epoch 010: Train MSE = 0.2960 (RMSE=0.5441), Val MSE = 0.3004 (RMSE=0.5481)\n",
      "Epoch 011: Train MSE = 0.2909 (RMSE=0.5394), Val MSE = 0.3110 (RMSE=0.5577)\n",
      "Early stopping triggered.\n",
      "California Results: {'best_epoch': 1, 'best_val_loss': 0.5238264870736026}\n"
     ]
    }
   ],
   "source": [
    "# california dataset\n",
    "data_loaders, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"california\", batch_size=256)\n",
    "model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "results = train_model(model, data_loaders[\"train\"], data_loaders[\"val\"], task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "print(\"California Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3569c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classification\n",
      "(880000, 28) (880000,)\n",
      "(220000, 28) (220000,)\n",
      "Epoch 001: Train Loss = 0.5427, Train Acc = 0.7179, Val Loss = 0.5266, Val Acc = 0.7300\n",
      "Epoch 002: Train Loss = 0.5225, Train Acc = 0.7334, Val Loss = 0.5152, Val Acc = 0.7401\n",
      "Epoch 003: Train Loss = 0.5157, Train Acc = 0.7384, Val Loss = 0.5085, Val Acc = 0.7427\n",
      "Epoch 004: Train Loss = 0.5094, Train Acc = 0.7429, Val Loss = 0.5032, Val Acc = 0.7480\n",
      "Epoch 005: Train Loss = 0.5045, Train Acc = 0.7460, Val Loss = 0.4999, Val Acc = 0.7518\n",
      "Epoch 006: Train Loss = 0.5008, Train Acc = 0.7486, Val Loss = 0.4978, Val Acc = 0.7517\n",
      "Epoch 007: Train Loss = 0.4980, Train Acc = 0.7504, Val Loss = 0.4948, Val Acc = 0.7524\n",
      "Epoch 008: Train Loss = 0.4952, Train Acc = 0.7528, Val Loss = 0.4953, Val Acc = 0.7533\n",
      "Epoch 009: Train Loss = 0.4928, Train Acc = 0.7542, Val Loss = 0.4909, Val Acc = 0.7557\n",
      "Epoch 010: Train Loss = 0.4910, Train Acc = 0.7554, Val Loss = 0.4915, Val Acc = 0.7550\n",
      "Epoch 011: Train Loss = 0.4889, Train Acc = 0.7566, Val Loss = 0.4880, Val Acc = 0.7578\n",
      "Epoch 012: Train Loss = 0.4872, Train Acc = 0.7583, Val Loss = 0.4895, Val Acc = 0.7568\n",
      "Epoch 013: Train Loss = 0.4858, Train Acc = 0.7589, Val Loss = 0.4860, Val Acc = 0.7597\n",
      "Epoch 014: Train Loss = 0.4841, Train Acc = 0.7604, Val Loss = 0.4892, Val Acc = 0.7584\n",
      "Epoch 015: Train Loss = 0.4827, Train Acc = 0.7616, Val Loss = 0.4852, Val Acc = 0.7602\n",
      "Epoch 016: Train Loss = 0.4814, Train Acc = 0.7617, Val Loss = 0.4869, Val Acc = 0.7589\n",
      "Epoch 017: Train Loss = 0.4802, Train Acc = 0.7623, Val Loss = 0.4833, Val Acc = 0.7607\n",
      "Epoch 018: Train Loss = 0.4786, Train Acc = 0.7636, Val Loss = 0.4822, Val Acc = 0.7620\n",
      "Epoch 019: Train Loss = 0.4778, Train Acc = 0.7642, Val Loss = 0.4827, Val Acc = 0.7625\n",
      "Epoch 020: Train Loss = 0.4767, Train Acc = 0.7652, Val Loss = 0.4819, Val Acc = 0.7624\n",
      "Epoch 021: Train Loss = 0.4751, Train Acc = 0.7662, Val Loss = 0.4811, Val Acc = 0.7631\n",
      "Epoch 022: Train Loss = 0.4744, Train Acc = 0.7666, Val Loss = 0.4805, Val Acc = 0.7631\n",
      "Epoch 023: Train Loss = 0.4734, Train Acc = 0.7672, Val Loss = 0.4832, Val Acc = 0.7615\n",
      "Epoch 024: Train Loss = 0.4725, Train Acc = 0.7678, Val Loss = 0.4820, Val Acc = 0.7634\n",
      "Epoch 025: Train Loss = 0.4715, Train Acc = 0.7685, Val Loss = 0.4819, Val Acc = 0.7632\n",
      "Epoch 026: Train Loss = 0.4704, Train Acc = 0.7690, Val Loss = 0.4822, Val Acc = 0.7639\n",
      "Epoch 027: Train Loss = 0.4695, Train Acc = 0.7696, Val Loss = 0.4807, Val Acc = 0.7647\n",
      "Epoch 028: Train Loss = 0.4682, Train Acc = 0.7706, Val Loss = 0.4821, Val Acc = 0.7645\n",
      "Epoch 029: Train Loss = 0.4677, Train Acc = 0.7707, Val Loss = 0.4806, Val Acc = 0.7655\n",
      "Epoch 030: Train Loss = 0.4667, Train Acc = 0.7718, Val Loss = 0.4784, Val Acc = 0.7651\n",
      "Epoch 031: Train Loss = 0.4657, Train Acc = 0.7722, Val Loss = 0.4792, Val Acc = 0.7653\n",
      "Epoch 032: Train Loss = 0.4647, Train Acc = 0.7730, Val Loss = 0.4799, Val Acc = 0.7644\n",
      "Epoch 033: Train Loss = 0.4641, Train Acc = 0.7730, Val Loss = 0.4772, Val Acc = 0.7658\n",
      "Epoch 034: Train Loss = 0.4629, Train Acc = 0.7739, Val Loss = 0.4786, Val Acc = 0.7659\n",
      "Epoch 035: Train Loss = 0.4624, Train Acc = 0.7741, Val Loss = 0.4798, Val Acc = 0.7660\n",
      "Epoch 036: Train Loss = 0.4609, Train Acc = 0.7753, Val Loss = 0.4803, Val Acc = 0.7655\n",
      "Epoch 037: Train Loss = 0.4603, Train Acc = 0.7755, Val Loss = 0.4778, Val Acc = 0.7659\n",
      "Epoch 038: Train Loss = 0.4596, Train Acc = 0.7759, Val Loss = 0.4780, Val Acc = 0.7655\n",
      "Epoch 039: Train Loss = 0.4586, Train Acc = 0.7766, Val Loss = 0.4776, Val Acc = 0.7672\n",
      "Epoch 040: Train Loss = 0.4578, Train Acc = 0.7771, Val Loss = 0.4779, Val Acc = 0.7662\n",
      "Epoch 041: Train Loss = 0.4570, Train Acc = 0.7774, Val Loss = 0.4783, Val Acc = 0.7673\n",
      "Epoch 042: Train Loss = 0.4560, Train Acc = 0.7787, Val Loss = 0.4790, Val Acc = 0.7666\n",
      "Epoch 043: Train Loss = 0.4555, Train Acc = 0.7785, Val Loss = 0.4822, Val Acc = 0.7655\n",
      "Epoch 044: Train Loss = 0.4549, Train Acc = 0.7792, Val Loss = 0.4780, Val Acc = 0.7671\n",
      "Epoch 045: Train Loss = 0.4539, Train Acc = 0.7794, Val Loss = 0.4811, Val Acc = 0.7664\n",
      "Epoch 046: Train Loss = 0.4528, Train Acc = 0.7802, Val Loss = 0.4810, Val Acc = 0.7657\n",
      "Epoch 047: Train Loss = 0.4521, Train Acc = 0.7811, Val Loss = 0.4818, Val Acc = 0.7653\n",
      "Epoch 048: Train Loss = 0.4514, Train Acc = 0.7810, Val Loss = 0.4825, Val Acc = 0.7655\n",
      "Epoch 049: Train Loss = 0.4505, Train Acc = 0.7814, Val Loss = 0.4859, Val Acc = 0.7640\n",
      "Epoch 050: Train Loss = 0.4497, Train Acc = 0.7820, Val Loss = 0.4839, Val Acc = 0.7648\n",
      "Epoch 051: Train Loss = 0.4491, Train Acc = 0.7826, Val Loss = 0.4817, Val Acc = 0.7646\n",
      "Early stopping triggered.\n",
      "Higgs Results: {'best_epoch': 41, 'best_val_acc': 0.7672545454545454}\n"
     ]
    }
   ],
   "source": [
    "# higgs dataset\n",
    "data_loaders, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"higgs\", batch_size=256)\n",
    "model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "results = train_model(model, data_loaders[\"train\"], data_loaders[\"val\"], task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "print(\"Higgs Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc99f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classification\n",
      "(227845, 30) (227845,)\n",
      "(56962, 30) (56962,)\n",
      "Epoch 001: Train Loss = 0.0066, Train Acc = 0.9992, Val Loss = 0.0030, Val Acc = 0.9995\n",
      "Epoch 002: Train Loss = 0.0035, Train Acc = 0.9994, Val Loss = 0.0031, Val Acc = 0.9994\n",
      "Epoch 003: Train Loss = 0.0034, Train Acc = 0.9993, Val Loss = 0.0030, Val Acc = 0.9994\n",
      "Epoch 004: Train Loss = 0.0033, Train Acc = 0.9994, Val Loss = 0.0043, Val Acc = 0.9990\n",
      "Epoch 005: Train Loss = 0.0032, Train Acc = 0.9994, Val Loss = 0.0026, Val Acc = 0.9995\n",
      "Epoch 006: Train Loss = 0.0031, Train Acc = 0.9993, Val Loss = 0.0030, Val Acc = 0.9992\n",
      "Epoch 007: Train Loss = 0.0031, Train Acc = 0.9994, Val Loss = 0.0028, Val Acc = 0.9994\n",
      "Epoch 008: Train Loss = 0.0031, Train Acc = 0.9994, Val Loss = 0.0025, Val Acc = 0.9995\n",
      "Epoch 009: Train Loss = 0.0030, Train Acc = 0.9994, Val Loss = 0.0026, Val Acc = 0.9995\n",
      "Epoch 010: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 011: Train Loss = 0.0030, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 012: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 013: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 014: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 015: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 016: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0025, Val Acc = 0.9995\n",
      "Epoch 017: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9994\n",
      "Epoch 018: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 019: Train Loss = 0.0029, Train Acc = 0.9994, Val Loss = 0.0022, Val Acc = 0.9995\n",
      "Epoch 020: Train Loss = 0.0028, Train Acc = 0.9994, Val Loss = 0.0024, Val Acc = 0.9995\n",
      "Epoch 021: Train Loss = 0.0028, Train Acc = 0.9994, Val Loss = 0.0027, Val Acc = 0.9994\n",
      "Early stopping triggered.\n",
      "Credit Card Results: {'best_epoch': 11, 'best_val_acc': 0.9995259997893332}\n"
     ]
    }
   ],
   "source": [
    "# creditcard dataset\n",
    "data_loaders, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"creditcard\", batch_size=256)\n",
    "model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "results = train_model(model, data_loaders[\"train\"], data_loaders[\"val\"], task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "print(\"Credit Card Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f83392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poker dataset\n",
    "data_loaders, n_cont, cat_cardinalities, d_out, task_type = prepare_data(\"poker\", batch_size=256)\n",
    "model = build_model(n_cont, cat_cardinalities, d_out)\n",
    "results = train_model(model, data_loaders[\"train\"], data_loaders[\"val\"], task_type, d_out, n_epochs=100, batch_size=256, patience=10, lr=3e-4)\n",
    "print(\"Poker Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
