{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93c9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 999\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d707a9c",
   "metadata": {},
   "source": [
    "adult_node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acaeb591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_13860\\3248147341.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
      "C:\\Windows\\Temp\\ipykernel_13860\\3248147341.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"income\"] = df[\"income\"].replace({\">50K.\": 1, \"<=50K.\": 0, \">50K\": 1, \"<=50K\": 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.4499 | Acc: 0.8542 | F1: 0.6669 | LR: 0.001000\n",
      "Epoch 10 | Loss: 0.2747 | Acc: 0.8533 | F1: 0.6507 | LR: 0.000500\n",
      "Epoch 15 | Loss: 0.3630 | Acc: 0.8529 | F1: 0.6588 | LR: 0.000500\n",
      "Epoch 20 | Loss: 0.3608 | Acc: 0.8536 | F1: 0.6534 | LR: 0.000250\n",
      "Epoch 25 | Loss: 0.4244 | Acc: 0.8528 | F1: 0.6574 | LR: 0.000250\n",
      "Epoch 30 | Loss: 0.2182 | Acc: 0.8531 | F1: 0.6562 | LR: 0.000125\n",
      "\n",
      "train_time: 36.51 seconds\n",
      "avg_cpu: 10.80%\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train_data = pd.read_csv('../data/adult/adult.data', header=None)\n",
    "test_data = pd.read_csv('../data/adult/adult.test', header=None, skiprows=1)\n",
    "\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "train_data.columns = test_data.columns = columns\n",
    "\n",
    "# cleaning data\n",
    "def clean_data(df):\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df[\"income\"] = df[\"income\"].replace({\">50K.\": 1, \"<=50K.\": 0, \">50K\": 1, \"<=50K\": 0})\n",
    "    return df\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "\n",
    "# label encoding\n",
    "X_train = train_data.drop(columns=[\"income\"])\n",
    "y_train = train_data[\"income\"]\n",
    "X_test = test_data.drop(columns=[\"income\"])\n",
    "y_test = test_data[\"income\"]\n",
    "\n",
    "# One-Hot Encoding \n",
    "X_combined = pd.get_dummies(pd.concat([X_train, X_test], axis=0))\n",
    "X_train = X_combined.iloc[:len(X_train)].copy().astype(np.float32)\n",
    "X_test = X_combined.iloc[len(X_train):].copy().astype(np.float32)\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# define the model\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# initialize the model, loss function, optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = NodeModel(input_dim=input_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# train the model\n",
    "start_time = time.time()\n",
    "cpu_usage_start = psutil.cpu_percent(interval=1)\n",
    "\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                output = model(batch_x)\n",
    "                pred_label = torch.argmax(output, dim=1)\n",
    "                all_preds.append(pred_label)\n",
    "                all_labels.append(batch_y)\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# evaluate the model\n",
    "end_time = time.time()\n",
    "cpu_usage_end = psutil.cpu_percent(interval=1)\n",
    "duration = end_time - start_time\n",
    "avg_cpu = (cpu_usage_start + cpu_usage_end) / 2\n",
    "\n",
    "print(f\"\\ntrain_time: {duration:.2f} seconds\")\n",
    "print(f\"avg_cpu: {avg_cpu:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95edb6",
   "metadata": {},
   "source": [
    "bank_marketing_node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67f4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_13860\\1234593227.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  bank_data = bank_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Loss: 23.9557 | Acc: 0.9122 | F1: 0.5745 | LR: 0.001000 | Time: 0.60s | CPU: 25.9%\n",
      "Epoch 10 | Loss: 22.9828 | Acc: 0.9154 | F1: 0.5794 | LR: 0.000500 | Time: 0.60s | CPU: 20.0%\n",
      "Epoch 15 | Loss: 22.2285 | Acc: 0.9154 | F1: 0.6117 | LR: 0.000500 | Time: 0.60s | CPU: 31.2%\n",
      "Epoch 20 | Loss: 21.8395 | Acc: 0.9155 | F1: 0.6032 | LR: 0.000250 | Time: 0.58s | CPU: 19.1%\n",
      "Epoch 25 | Loss: 21.8064 | Acc: 0.9144 | F1: 0.5865 | LR: 0.000250 | Time: 0.62s | CPU: 41.7%\n",
      "Epoch 30 | Loss: 21.5286 | Acc: 0.9158 | F1: 0.5970 | LR: 0.000125 | Time: 0.61s | CPU: 18.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read bank data\n",
    "bank_data = pd.read_csv('../data/bank_marketing/bank_additional_full.csv', sep=';')\n",
    "bank_data = bank_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "bank_data['y'] = bank_data['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "\n",
    "X = pd.get_dummies(bank_data.drop(columns=['y']))\n",
    "y = bank_data['y']\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=256, shuffle=True)\n",
    "\n",
    "# define the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = NodeModel(input_dim=input_dim)  # 重用你的 NodeModel\n",
    "\n",
    "# initialize the model, loss function, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(30):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test_tensor)\n",
    "        pred_label = torch.argmax(pred, dim=1)\n",
    "        acc = accuracy_score(y_test_tensor, pred_label)\n",
    "        f1 = f1_score(y_test_tensor, pred_label)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        cpu = psutil.cpu_percent(interval=0.1)\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06794d",
   "metadata": {},
   "source": [
    "covertype_node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1889df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Loss: 998.3166 | Acc: 0.7852 | F1: 0.5762 | LR: 0.001000 | Time: 8.16s | CPU: 16.3%\n",
      "Epoch 10 | Loss: 954.9634 | Acc: 0.7997 | F1: 0.6518 | LR: 0.000500 | Time: 8.10s | CPU: 18.4%\n",
      "Epoch 15 | Loss: 927.3428 | Acc: 0.8088 | F1: 0.6806 | LR: 0.000500 | Time: 8.18s | CPU: 16.7%\n",
      "Epoch 20 | Loss: 920.6391 | Acc: 0.8138 | F1: 0.6830 | LR: 0.000250 | Time: 8.38s | CPU: 21.0%\n",
      "Epoch 25 | Loss: 907.8465 | Acc: 0.8166 | F1: 0.6943 | LR: 0.000250 | Time: 10.10s | CPU: 22.0%\n",
      "Epoch 30 | Loss: 904.5499 | Acc: 0.8169 | F1: 0.6903 | LR: 0.000125 | Time: 9.15s | CPU: 22.6%\n"
     ]
    }
   ],
   "source": [
    "# read  data\n",
    "covtype_data = pd.read_csv('../data/covertype/covtype.data', header=None)\n",
    "\n",
    "\n",
    "X = covtype_data.iloc[:, :-1]\n",
    "y = covtype_data.iloc[:, -1] - 1  \n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=256, shuffle=True)\n",
    "\n",
    "# define the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = len(np.unique(y_train))  # == 7\n",
    "model = NodeModel(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "# initialize the model, loss function, optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(30):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test_tensor)\n",
    "        pred_label = torch.argmax(pred, dim=1)\n",
    "        acc = accuracy_score(y_test_tensor, pred_label)\n",
    "        f1 = f1_score(y_test_tensor, pred_label, average='macro')  # Modify here for multiclass F1 score\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        cpu = psutil.cpu_percent(interval=0.1)\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {total_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c671f3",
   "metadata": {},
   "source": [
    "小样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7367a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 10% of the data...\n",
      "Epoch 5 | Loss: 0.6275 | Train Acc: 0.7329 | Test Acc: 0.7469 | LR: 0.001000\n",
      "Epoch 10 | Loss: 0.5861 | Train Acc: 0.7490 | Test Acc: 0.7603 | LR: 0.001000\n",
      "Epoch 15 | Loss: 0.5612 | Train Acc: 0.7586 | Test Acc: 0.7727 | LR: 0.000500\n",
      "Epoch 20 | Loss: 0.5537 | Train Acc: 0.7613 | Test Acc: 0.7743 | LR: 0.000500\n",
      "Epoch 25 | Loss: 0.5443 | Train Acc: 0.7657 | Test Acc: 0.7798 | LR: 0.000250\n",
      "Epoch 30 | Loss: 0.5372 | Train Acc: 0.7654 | Test Acc: 0.7789 | LR: 0.000250\n",
      "\n",
      "Training with 50% of the data...\n",
      "Epoch 5 | Loss: 0.5591 | Train Acc: 0.7581 | Test Acc: 0.7793 | LR: 0.001000\n",
      "Epoch 10 | Loss: 0.5360 | Train Acc: 0.7684 | Test Acc: 0.7931 | LR: 0.001000\n",
      "Epoch 15 | Loss: 0.5185 | Train Acc: 0.7782 | Test Acc: 0.8037 | LR: 0.000500\n",
      "Epoch 20 | Loss: 0.5128 | Train Acc: 0.7810 | Test Acc: 0.8084 | LR: 0.000500\n",
      "Epoch 25 | Loss: 0.5040 | Train Acc: 0.7841 | Test Acc: 0.8116 | LR: 0.000250\n",
      "Epoch 30 | Loss: 0.5028 | Train Acc: 0.7856 | Test Acc: 0.8121 | LR: 0.000250\n",
      "\n",
      "Training with 100% of the data...\n",
      "Epoch 5 | Loss: 0.5394 | Train Acc: 0.7675 | Test Acc: 0.7938 | LR: 0.001000\n",
      "Epoch 10 | Loss: 0.5213 | Train Acc: 0.7743 | Test Acc: 0.8083 | LR: 0.001000\n",
      "Epoch 15 | Loss: 0.5028 | Train Acc: 0.7838 | Test Acc: 0.8125 | LR: 0.000500\n",
      "Epoch 20 | Loss: 0.4994 | Train Acc: 0.7851 | Test Acc: 0.8201 | LR: 0.000500\n",
      "Epoch 25 | Loss: 0.4919 | Train Acc: 0.7890 | Test Acc: 0.8213 | LR: 0.000250\n",
      "Epoch 30 | Loss: 0.4895 | Train Acc: 0.7894 | Test Acc: 0.8243 | LR: 0.000250\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "covtype_data = pd.read_csv('../data/covertype/covtype.data', header=None)\n",
    "columns = [f'feature_{i}' for i in range(1, 55)] + ['target']\n",
    "covtype_data.columns = columns\n",
    "\n",
    "# label encoding\n",
    "X = covtype_data.drop(columns=['target'])\n",
    "y = covtype_data['target'] - 1  # 使标签从 0 开始\n",
    "\n",
    "# train-test split\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# rtansform to tensor\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# define the model\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=7, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# define the training function\n",
    "def train_model(X_train, y_train, X_test, y_test, batch_size, lr_scheduler=None):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = NodeModel(input_dim=X_train.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # uesr-defined learning rate scheduler\n",
    "    if lr_scheduler:\n",
    "        scheduler = lr_scheduler(optimizer, step_size=10, gamma=0.5)  \n",
    "\n",
    "    EPOCHS = 30\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "            total_preds += target.size(0)\n",
    "\n",
    "        # calculate average loss and accuracy\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct_preds / total_preds\n",
    "\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct_preds = 0\n",
    "                total_preds = 0\n",
    "                for data, target in test_loader:\n",
    "                    output = model(data)\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    correct_preds += (predicted == target).sum().item()\n",
    "                    total_preds += target.size(0)\n",
    "\n",
    "                test_acc = correct_preds / total_preds\n",
    "                if lr_scheduler:\n",
    "                    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1} | Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "        if lr_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "# try with different sample sizes\n",
    "sample_sizes = [0.1, 0.5]  # 1.0\n",
    "batch_size = 64\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR  \n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    print(f\"\\nTraining with {int(sample_size*100)}% of the data...\")\n",
    "\n",
    "    # select a subset of the data\n",
    "    X_sub, _, y_sub, _ = train_test_split(X_tensor, y_tensor, train_size=sample_size, random_state=42)\n",
    "\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_sub, y_sub, train_size=0.8, random_state=42)\n",
    "\n",
    "    train_model(X_train_sub, y_train_sub, X_test_sub, y_test_sub, batch_size, lr_scheduler)\n",
    "\n",
    "# train with 100% of the data\n",
    "print(\"\\nTraining with 100% of the data...\")\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_tensor, y_tensor, train_size=0.8, random_state=42)\n",
    "train_model(X_train_full, y_train_full, X_test_full, y_test_full, batch_size, lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bffba9",
   "metadata": {},
   "source": [
    "poker_hand_node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62701c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Loss: 96.7761 | Acc: 0.5223 | F1: 0.4135 | LR: 0.001000 | Time: 1.31s | CPU: 12.5%\n",
      "Epoch 10 | Loss: 94.8041 | Acc: 0.5453 | F1: 0.4935 | LR: 0.000500 | Time: 1.31s | CPU: 1.8%\n",
      "Epoch 15 | Loss: 94.3800 | Acc: 0.5446 | F1: 0.4874 | LR: 0.000500 | Time: 1.28s | CPU: 10.2%\n",
      "Epoch 20 | Loss: 94.0692 | Acc: 0.5468 | F1: 0.4985 | LR: 0.000250 | Time: 1.32s | CPU: 7.1%\n",
      "Epoch 25 | Loss: 93.8110 | Acc: 0.5466 | F1: 0.4968 | LR: 0.000250 | Time: 1.23s | CPU: 7.3%\n",
      "Epoch 30 | Loss: 93.6370 | Acc: 0.5467 | F1: 0.4956 | LR: 0.000125 | Time: 1.19s | CPU: 15.6%\n"
     ]
    }
   ],
   "source": [
    "ph_testing_data = pd.read_csv('../data/poker_hand/poker_hand_testing.data', header=None)\n",
    "ph_trainingtrue_data = pd.read_csv('../data/poker_hand/poker_hand_training-true.data', header=None)\n",
    "\n",
    "columns = [f'feature_{i}' for i in range(1, 11)] + ['target']\n",
    "ph_testing_data.columns = columns\n",
    "ph_trainingtrue_data.columns = columns\n",
    "\n",
    "X_train = ph_trainingtrue_data.drop(columns=['target'])\n",
    "y_train = ph_trainingtrue_data['target']\n",
    "X_test = ph_testing_data.drop(columns=['target'])\n",
    "y_test = ph_testing_data['target']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# define the model\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=10, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = NodeModel(input_dim=input_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# train the model\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test_tensor)\n",
    "        pred_label = torch.argmax(pred, dim=1)\n",
    "        acc = accuracy_score(y_test_tensor, pred_label)\n",
    "        f1 = f1_score(y_test_tensor, pred_label, average='weighted')\n",
    "\n",
    "    end_time = time.time()\n",
    "    cpu = psutil.cpu_percent(interval=0.1)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {end_time - start_time:.2f}s | CPU: {cpu}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346c0df",
   "metadata": {},
   "source": [
    "wine_node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88554d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training on Red Wine Dataset ---\n",
      "Epoch  5 | Loss: 9.9527 | Acc: 0.5572 | F1: 0.4961 | LR: 0.001000 | Time: 0.12s | CPU: 43.4%\n",
      "Epoch 10 | Loss: 7.6873 | Acc: 0.5941 | F1: 0.5662 | LR: 0.000500 | Time: 0.15s | CPU: 26.7%\n",
      "Epoch 15 | Loss: 7.4430 | Acc: 0.5947 | F1: 0.5659 | LR: 0.000500 | Time: 0.12s | CPU: 27.8%\n",
      "Epoch 20 | Loss: 7.3417 | Acc: 0.5960 | F1: 0.5703 | LR: 0.000250 | Time: 0.14s | CPU: 20.4%\n",
      "Epoch 25 | Loss: 7.1813 | Acc: 0.5985 | F1: 0.5734 | LR: 0.000250 | Time: 0.14s | CPU: 18.2%\n",
      "Epoch 30 | Loss: 7.1662 | Acc: 0.5966 | F1: 0.5715 | LR: 0.000125 | Time: 0.14s | CPU: 21.8%\n",
      "\n",
      "--- Training on White Wine Dataset ---\n",
      "Epoch  5 | Loss: 24.6664 | Acc: 0.5310 | F1: 0.4649 | LR: 0.001000 | Time: 0.18s | CPU: 16.7%\n",
      "Epoch 10 | Loss: 22.5499 | Acc: 0.5498 | F1: 0.5160 | LR: 0.000500 | Time: 0.20s | CPU: 23.6%\n",
      "Epoch 15 | Loss: 22.5320 | Acc: 0.5551 | F1: 0.5218 | LR: 0.000500 | Time: 0.20s | CPU: 27.1%\n",
      "Epoch 20 | Loss: 22.0785 | Acc: 0.5570 | F1: 0.5239 | LR: 0.000250 | Time: 0.20s | CPU: 21.9%\n",
      "Epoch 25 | Loss: 21.9493 | Acc: 0.5619 | F1: 0.5309 | LR: 0.000250 | Time: 0.18s | CPU: 21.4%\n",
      "Epoch 30 | Loss: 22.1886 | Acc: 0.5600 | F1: 0.5267 | LR: 0.000125 | Time: 0.17s | CPU: 38.0%\n"
     ]
    }
   ],
   "source": [
    "# read wine data\n",
    "wine_red_data = pd.read_csv('../data/wine_quality/winequality_red.csv', sep=';', header=0)\n",
    "wine_white_data = pd.read_csv('../data/wine_quality/winequality_white.csv', sep=';', header=0)\n",
    "\n",
    "# cleaning data\n",
    "wine_red_data = wine_red_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "wine_white_data = wine_white_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "# label encoding\n",
    "X_red = wine_red_data.drop(columns=['quality'])\n",
    "y_red = wine_red_data['quality']\n",
    "X_white = wine_white_data.drop(columns=['quality'])\n",
    "y_white = wine_white_data['quality']\n",
    "\n",
    "# standardization\n",
    "scaler_red = StandardScaler()\n",
    "X_red_scaled = scaler_red.fit_transform(X_red)\n",
    "scaler_white = StandardScaler()\n",
    "X_white_scaled = scaler_white.fit_transform(X_white)\n",
    "\n",
    "# transform to tensor\n",
    "X_red_tensor = torch.tensor(X_red_scaled, dtype=torch.float32)\n",
    "y_red_tensor = torch.tensor(y_red.values, dtype=torch.long)\n",
    "X_white_tensor = torch.tensor(X_white_scaled, dtype=torch.float32)\n",
    "y_white_tensor = torch.tensor(y_white.values, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "BATCH_SIZE = 256\n",
    "train_loader_red = DataLoader(TensorDataset(X_red_tensor, y_red_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_white = DataLoader(TensorDataset(X_white_tensor, y_white_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# define the model\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=11, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(X_tensor, y_tensor, loader, label='Red Wine'):\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = NodeModel(input_dim=input_dim, output_dim=11)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    print(f\"\\n--- Training on {label} Dataset ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor)\n",
    "            pred_label = torch.argmax(pred, dim=1)\n",
    "            acc = accuracy_score(y_tensor, pred_label)\n",
    "            f1 = f1_score(y_tensor, pred_label, average='weighted')\n",
    "\n",
    "        cpu = psutil.cpu_percent(interval=0.1)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n",
    "\n",
    "# train the model on both datasets\n",
    "train_model(X_red_tensor, y_red_tensor, train_loader_red, label='Red Wine')\n",
    "train_model(X_white_tensor, y_white_tensor, train_loader_white, label='White Wine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85e8fe",
   "metadata": {},
   "source": [
    "California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2068422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Loss: 42.9162 | RMSE: 0.7011 | LR: 0.001000 | Time: 0.34s | CPU: 30.4%\n",
      "Epoch 10 | Loss: 36.8705 | RMSE: 0.6592 | LR: 0.000500 | Time: 0.36s | CPU: 25.0%\n",
      "Epoch 15 | Loss: 33.6907 | RMSE: 0.6417 | LR: 0.000500 | Time: 0.37s | CPU: 29.8%\n",
      "Epoch 20 | Loss: 31.8468 | RMSE: 0.6303 | LR: 0.000250 | Time: 0.36s | CPU: 27.9%\n",
      "Epoch 25 | Loss: 31.2817 | RMSE: 0.6242 | LR: 0.000250 | Time: 0.38s | CPU: 17.9%\n",
      "Epoch 30 | Loss: 30.1810 | RMSE: 0.6188 | LR: 0.000125 | Time: 0.35s | CPU: 23.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "# Load California Housing dataset\n",
    "california_data = fetch_california_housing()\n",
    "\n",
    "# Extract features and target\n",
    "X = california_data.data\n",
    "y = california_data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Convert target to column vector\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define the model (for regression)\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=1, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = NodeModel(input_dim=input_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # For regression, use MSELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test_tensor)\n",
    "        mse = criterion(pred, y_test_tensor)\n",
    "        rmse = torch.sqrt(mse)  # RMSE\n",
    "\n",
    "    cpu = psutil.cpu_percent(interval=0.1)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | RMSE: {rmse.item():.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f56374",
   "metadata": {},
   "source": [
    "HIGGS_node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242574d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Loss: 73650.1854 | Acc: 0.7413 | F1: 0.7547 | LR: 0.001000 | Time: 296.09s | CPU: 1.8%\n",
      "Epoch 10 | Loss: 73509.3767 | Acc: 0.7433 | F1: 0.7557 | LR: 0.000500 | Time: 311.60s | CPU: 0.0%\n",
      "Epoch 15 | Loss: 72969.6374 | Acc: 0.7452 | F1: 0.7653 | LR: 0.000500 | Time: 307.21s | CPU: 0.0%\n",
      "Epoch 20 | Loss: 72931.0982 | Acc: 0.7454 | F1: 0.7617 | LR: 0.000250 | Time: 307.54s | CPU: 0.0%\n",
      "Epoch 25 | Loss: 72668.1137 | Acc: 0.7461 | F1: 0.7667 | LR: 0.000250 | Time: 337.17s | CPU: 3.3%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_y)\n\u001b[0;32m     62\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 63\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     66\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    235\u001b[0m         group,\n\u001b[0;32m    236\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m         state_steps,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[1;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python作业\\lib\\site-packages\\torch\\optim\\adam.py:397\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m    395\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    398\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read wine data\n",
    "HIGGS_data = pd.read_csv('../data/HIGGS/HIGGS.csv', header=None)\n",
    "\n",
    "X = HIGGS_data.iloc[:, 1:].values  \n",
    "y = HIGGS_data.iloc[:, 0].values   \n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# define the model (for binary classification)\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# initialize the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = NodeModel(input_dim=input_dim)\n",
    "\n",
    "# initialize the model, loss function, optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# train the model\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test_tensor)\n",
    "        pred_label = torch.argmax(pred, dim=1)\n",
    "        acc = accuracy_score(y_test_tensor, pred_label)\n",
    "        f1 = f1_score(y_test_tensor, pred_label)\n",
    "\n",
    "    cpu = psutil.cpu_percent(interval=0.1)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f7bc4",
   "metadata": {},
   "source": [
    "Telco Customer Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ff8b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training on Telco Customer Churn Dataset ---\n",
      "Epoch  5 | Loss: 0.0384 | Acc: 1.0000 | F1: 1.0000 | LR: 0.001000 | Time: 0.28s | CPU: 26.3%\n",
      "Epoch 10 | Loss: 0.0095 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000500 | Time: 0.26s | CPU: 41.1%\n",
      "Epoch 15 | Loss: 0.0063 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000500 | Time: 0.25s | CPU: 36.5%\n",
      "Epoch 20 | Loss: 0.0044 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000250 | Time: 0.23s | CPU: 28.6%\n",
      "Epoch 25 | Loss: 0.0034 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000250 | Time: 0.35s | CPU: 32.6%\n",
      "Epoch 30 | Loss: 0.0031 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000125 | Time: 0.23s | CPU: 21.1%\n"
     ]
    }
   ],
   "source": [
    "# Load Telco Customer Churn dataset\n",
    "TC_data = pd.read_csv('../data/Telco Customer Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Inspect and clean the dataset (handle missing values, encode categorical variables)\n",
    "# Drop any unnecessary columns if needed (e.g., customerID) and convert to appropriate types\n",
    "TC_data = TC_data.drop(columns=['customerID'])\n",
    "\n",
    "# Convert categorical columns using label encoding or one-hot encoding\n",
    "categorical_cols = TC_data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    TC_data[col] = TC_data[col].astype('category').cat.codes  # Label encoding\n",
    "\n",
    "# Split the data into features and target\n",
    "X_TC = TC_data.drop(columns=['Churn'])\n",
    "y_TC = TC_data['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)  # Convert Churn to binary\n",
    "\n",
    "# Standardize the features\n",
    "scaler_TC = StandardScaler()\n",
    "X_TC_scaled = scaler_TC.fit_transform(X_TC)\n",
    "\n",
    "# Convert to tensors\n",
    "X_TC_tensor = torch.tensor(X_TC_scaled, dtype=torch.float32)\n",
    "y_TC_tensor = torch.tensor(y_TC.values, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader\n",
    "BATCH_SIZE = 256\n",
    "train_loader_TC = DataLoader(TensorDataset(X_TC_tensor, y_TC_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define the model (same as the NodeModel used for the wine dataset)\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train the model on the Telco Customer Churn dataset\n",
    "def train_model(X_tensor, y_tensor, loader, label='Telco Customer Churn'):\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = NodeModel(input_dim=input_dim, output_dim=2)  # Binary classification\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    print(f\"\\n--- Training on {label} Dataset ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor)\n",
    "            pred_label = torch.argmax(pred, dim=1)\n",
    "            acc = accuracy_score(y_tensor, pred_label)\n",
    "            f1 = f1_score(y_tensor, pred_label, average='weighted')\n",
    "\n",
    "        cpu = psutil.cpu_percent(interval=0.1)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n",
    "\n",
    "# Train the model on the Telco dataset\n",
    "train_model(X_TC_tensor, y_TC_tensor, train_loader_TC, label='Telco Customer Churn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training on Telco Customer Churn Dataset ---\n",
      "Epoch  5 | Loss: 0.7081 | Acc: 1.0000 | F1: 1.0000 | LR: 0.001000 | Time: 1.85s | CPU: 25.0%\n",
      "Epoch 10 | Loss: 0.0874 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000500 | Time: 1.73s | CPU: 30.9%\n",
      "Epoch 15 | Loss: 0.0623 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000500 | Time: 1.95s | CPU: 22.2%\n",
      "Epoch 20 | Loss: 0.0261 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000250 | Time: 2.15s | CPU: 24.4%\n",
      "Epoch 25 | Loss: 0.0237 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000250 | Time: 1.83s | CPU: 36.8%\n",
      "Epoch 30 | Loss: 0.0219 | Acc: 1.0000 | F1: 1.0000 | LR: 0.000125 | Time: 1.93s | CPU: 39.7%\n"
     ]
    }
   ],
   "source": [
    "# read wine data\n",
    "TC_data = pd.read_csv('../data/Telco Customer Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Inspect and clean the dataset (handle missing values, encode categorical variables)\n",
    "# Drop any unnecessary columns if needed (e.g., customerID) and convert to appropriate types\n",
    "numerical_columns = TC_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "TC_data[numerical_columns] = TC_data[numerical_columns].fillna(TC_data[numerical_columns].mean())\n",
    "\n",
    "# drop any rows with missing values in the target column\n",
    "categorical_columns = TC_data.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    TC_data[column] = TC_data[column].fillna(TC_data[column].mode()[0])\n",
    "\n",
    "if 'Churn' in TC_data.columns:\n",
    "    TC_data['Churn'] = TC_data['Churn'].fillna(TC_data['Churn'].mode()[0])\n",
    "\n",
    "if TC_data['Churn'].dtype == 'object':\n",
    "    TC_data['Churn'] = TC_data['Churn'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "\n",
    "X_TC = TC_data.drop(columns=['Churn'])\n",
    "y_TC = TC_data['Churn']\n",
    "\n",
    "X_TC = pd.get_dummies(X_TC, drop_first=True)  # One-hot encoding\n",
    "X_TC = X_TC.astype('float32')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_TC_scaled = scaler.fit_transform(X_TC)\n",
    "\n",
    "\n",
    "if not np.issubdtype(X_TC_scaled.dtype, np.number):\n",
    "    raise ValueError(\"X_TC_scaled contains non-numeric values. Please check the preprocessing steps.\")\n",
    "\n",
    "\n",
    "if not np.issubdtype(y_TC.values.dtype, np.number):\n",
    "    raise ValueError(\"y_TC contains non-numeric values. Please check the preprocessing steps.\")\n",
    "\n",
    "X_TC_tensor = torch.tensor(X_TC_scaled, dtype=torch.float32)\n",
    "y_TC_tensor = torch.tensor(y_TC.values, dtype=torch.long)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(TensorDataset(X_TC_tensor, y_TC_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(X_tensor, y_tensor, loader, label='Telco Customer Churn'):\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = NodeModel(input_dim=input_dim, output_dim=2)  # 2类（Churn or Not Churn）\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    print(f\"\\n--- Training on {label} Dataset ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor)\n",
    "            pred_label = torch.argmax(pred, dim=1)\n",
    "            acc = accuracy_score(y_tensor, pred_label)\n",
    "            f1 = f1_score(y_tensor, pred_label, average='weighted')\n",
    "\n",
    "        cpu = psutil.cpu_percent(interval=0.1)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n",
    "\n",
    "train_model(X_TC_tensor, y_TC_tensor, train_loader, label='Telco Customer Churn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b5444",
   "metadata": {},
   "source": [
    "CreditcCard Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481711ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "                Time            V1            V2            V3            V4  \\\n",
      "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
      "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
      "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
      "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
      "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
      "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
      "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
      "\n",
      "                 V5            V6            V7            V8            V9  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
      "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
      "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
      "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
      "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
      "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
      "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
      "\n",
      "       ...           V21           V22           V23           V24  \\\n",
      "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
      "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
      "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
      "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
      "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
      "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
      "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
      "\n",
      "                V25           V26           V27           V28         Amount  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
      "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
      "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
      "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
      "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
      "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
      "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
      "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
      "\n",
      "               Class  \n",
      "count  284807.000000  \n",
      "mean        0.001727  \n",
      "std         0.041527  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 31 columns]\n",
      "\n",
      "--- Training on Credit Card Fraud Detection Dataset ---\n",
      "Epoch  5 | Loss: 3.3012 | Acc: 0.9995 | F1: 0.9994 | LR: 0.001000 | Time: 5.61s | CPU: 14.6%\n",
      "Epoch 10 | Loss: 2.9749 | Acc: 0.9995 | F1: 0.9995 | LR: 0.000500 | Time: 7.18s | CPU: 12.3%\n",
      "Epoch 15 | Loss: 2.4468 | Acc: 0.9996 | F1: 0.9996 | LR: 0.000500 | Time: 6.02s | CPU: 26.8%\n",
      "Epoch 20 | Loss: 2.3224 | Acc: 0.9996 | F1: 0.9996 | LR: 0.000250 | Time: 7.17s | CPU: 12.5%\n",
      "Epoch 25 | Loss: 2.1475 | Acc: 0.9996 | F1: 0.9996 | LR: 0.000250 | Time: 7.35s | CPU: 12.7%\n",
      "Epoch 30 | Loss: 1.9034 | Acc: 0.9997 | F1: 0.9997 | LR: 0.000125 | Time: 5.35s | CPU: 29.7%\n"
     ]
    }
   ],
   "source": [
    "# Load Credit Card Fraud Detection dataset\n",
    "CC_data = pd.read_csv('../data/Credit Card Fraud Detection/creditcard.csv')\n",
    "\n",
    "# Inspect the data (e.g., check for missing values, imbalanced classes)\n",
    "print(CC_data.info())\n",
    "print(CC_data.describe())\n",
    "\n",
    "# Drop the 'Time' column and separate features and target variable\n",
    "X_CC = CC_data.drop(columns=['Class', 'Time'])\n",
    "y_CC = CC_data['Class']  # 'Class' is the target variable (fraud = 1, non-fraud = 0)\n",
    "\n",
    "# Standardize the features\n",
    "scaler_CC = StandardScaler()\n",
    "X_CC_scaled = scaler_CC.fit_transform(X_CC)\n",
    "\n",
    "# Convert to tensors\n",
    "X_CC_tensor = torch.tensor(X_CC_scaled, dtype=torch.float32)\n",
    "y_CC_tensor = torch.tensor(y_CC.values, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader\n",
    "BATCH_SIZE = 256\n",
    "train_loader_CC = DataLoader(TensorDataset(X_CC_tensor, y_CC_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define the model (same as the NodeModel used for the wine dataset)\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train the model on the Credit Card Fraud dataset\n",
    "def train_model(X_tensor, y_tensor, loader, label='Credit Card Fraud Detection'):\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = NodeModel(input_dim=input_dim, output_dim=2)  # Binary classification (fraud = 1, non-fraud = 0)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    print(f\"\\n--- Training on {label} Dataset ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor)\n",
    "            pred_label = torch.argmax(pred, dim=1)\n",
    "            acc = accuracy_score(y_tensor, pred_label)\n",
    "            f1 = f1_score(y_tensor, pred_label, average='weighted')\n",
    "\n",
    "        cpu = psutil.cpu_percent(interval=0.1)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | Loss: {epoch_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {elapsed:.2f}s | CPU: {cpu}%\")\n",
    "\n",
    "# Train the model on the Credit Card Fraud dataset\n",
    "train_model(X_CC_tensor, y_CC_tensor, train_loader_CC, label='Credit Card Fraud Detection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4ad14",
   "metadata": {},
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_csv('../data/adult/adult.data', header=None)\n",
    "test_data = pd.read_csv('../data/adult/adult.test', header=None, skiprows=1)\n",
    "\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "train_data.columns = test_data.columns = columns\n",
    "\n",
    "# cleaning data\n",
    "def clean_data(df):\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df[\"income\"] = df[\"income\"].replace({\">50K.\": 1, \"<=50K.\": 0, \">50K\": 1, \"<=50K\": 0})\n",
    "    return df\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "\n",
    "# label encoding\n",
    "X_train = train_data.drop(columns=[\"income\"])\n",
    "y_train = train_data[\"income\"]\n",
    "X_test = test_data.drop(columns=[\"income\"])\n",
    "y_test = test_data[\"income\"]\n",
    "\n",
    "# One-Hot Encoding \n",
    "X_combined = pd.get_dummies(pd.concat([X_train, X_test], axis=0))\n",
    "X_train = X_combined.iloc[:len(X_train)].copy().astype(np.float32)\n",
    "X_test = X_combined.iloc[len(X_train):].copy().astype(np.float32)\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# define the model\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train the model for a given seed\n",
    "def train_model_with_seed(seed):\n",
    "    # Set random seed for reproducibility\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    # Re-initialize model, optimizer, and scheduler\n",
    "    model = NodeModel(input_dim=X_train_tensor.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    start_time = time.time()\n",
    "    cpu_usage_start = psutil.cpu_percent(interval=1)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in test_loader:\n",
    "                    output = model(batch_x)\n",
    "                    pred_label = torch.argmax(output, dim=1)\n",
    "                    all_preds.append(pred_label)\n",
    "                    all_labels.append(batch_y)\n",
    "\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "            print(f\"Seed {seed} | Epoch {epoch+1} | Loss: {loss.item():.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    end_time = time.time()\n",
    "    cpu_usage_end = psutil.cpu_percent(interval=1)\n",
    "    duration = end_time - start_time\n",
    "    avg_cpu = (cpu_usage_start + cpu_usage_end) / 2\n",
    "\n",
    "    print(f\"\\nSeed {seed} | train_time: {duration:.2f} seconds\")\n",
    "    print(f\"Seed {seed} | avg_cpu: {avg_cpu:.2f}%\")\n",
    "\n",
    "# Run the training for different seeds\n",
    "for seed_value in [999, 888, 777]:\n",
    "    train_model_with_seed(seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_csv('../data/adult/adult.data', header=None)\n",
    "test_data = pd.read_csv('../data/adult/adult.test', header=None, skiprows=1)\n",
    "\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "train_data.columns = test_data.columns = columns\n",
    "\n",
    "# cleaning data\n",
    "def clean_data(df):\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df[\"income\"] = df[\"income\"].replace({\">50K.\": 1, \"<=50K.\": 0, \">50K\": 1, \"<=50K\": 0})\n",
    "    return df\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)\n",
    "\n",
    "# label encoding\n",
    "X_train = train_data.drop(columns=[\"income\"])\n",
    "y_train = train_data[\"income\"]\n",
    "X_test = test_data.drop(columns=[\"income\"])\n",
    "y_test = test_data[\"income\"]\n",
    "\n",
    "# One-Hot Encoding \n",
    "X_combined = pd.get_dummies(pd.concat([X_train, X_test], axis=0))\n",
    "X_train = X_combined.iloc[:len(X_train)].copy().astype(np.float32)\n",
    "X_test = X_combined.iloc[len(X_train):].copy().astype(np.float32)\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# transform to tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# create dataset and dataloader\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# define the model\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=2, dropout=0.3):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train the model for a given seed\n",
    "def train_model_with_seed(seed):\n",
    "    # Set random seed for reproducibility\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    # Re-initialize model, optimizer, and scheduler\n",
    "    model = NodeModel(input_dim=X_train_tensor.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    start_time = time.time()\n",
    "    cpu_usage_start = psutil.cpu_percent(interval=1)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in test_loader:\n",
    "                    output = model(batch_x)\n",
    "                    pred_label = torch.argmax(output, dim=1)\n",
    "                    all_preds.append(pred_label)\n",
    "                    all_labels.append(batch_y)\n",
    "\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "            print(f\"Seed {seed} | Epoch {epoch+1} | Loss: {loss.item():.4f} | Acc: {acc:.4f} | F1: {f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    end_time = time.time()\n",
    "    cpu_usage_end = psutil.cpu_percent(interval=1)\n",
    "    duration = end_time - start_time\n",
    "    avg_cpu = (cpu_usage_start + cpu_usage_end) / 2\n",
    "\n",
    "    print(f\"\\nSeed {seed} | train_time: {duration:.2f} seconds\")\n",
    "    print(f\"Seed {seed} | avg_cpu: {avg_cpu:.2f}%\")\n",
    "\n",
    "# Run the training for different seeds\n",
    "for seed_value in [999, 888, 777]:\n",
    "    train_model_with_seed(seed_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
